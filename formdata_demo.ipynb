{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7330e923-c803-4324-9d4f-af5c95e982ca",
   "metadata": {},
   "source": [
    "## 機械学習：野球のスイング判定\n",
    "\n",
    "* 野球のバッティングフォームを対象とする\n",
    "* 良いスイングのパターンを学習させて判定できるようにしたい\n",
    "\n",
    "### 今回用いる方法（概要）\n",
    "\n",
    "1. 姿勢推定により身体の部位の位置を取得\n",
    "2. 動画からスイング部分を切り出す（本Notebook内では取り扱わない）\n",
    "3. 時系列に位置を並べてデータ化する\n",
    "4. 良いパターンと良くないパターンについて学習する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeb893d-7e4e-427d-b922-ff7f37e6c109",
   "metadata": {},
   "source": [
    "### 準備：動画の処理（再生）\n",
    "\n",
    "* OpenCV.VideoCapture\n",
    "* 1フレームずつ表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12eaad1-2ed1-4d1d-8e83-afa208bb11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, Image, clear_output\n",
    "from pprint import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a9bc0-ea20-4445-b9cd-770d10d17a36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imshow(img, width=240):\n",
    "    _, buf = cv2.imencode(\".jpg\", img)\n",
    "    display(Image(data=buf.tobytes(), width=width))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "class CvCapture():\n",
    "    def __init__(self, video_path):\n",
    "        self.cap = cv2.VideoCapture(video_path)\n",
    "        self.fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    def read(self):\n",
    "        return self.cap.read()\n",
    "    \n",
    "    def get_specified_frame(self, frame_number):\n",
    "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number - 1)\n",
    "        ret, frame = self.cap.read()\n",
    "        return frame\n",
    "\n",
    "    def release(self):\n",
    "        self.cap.release()\n",
    "\n",
    "    def process_frames(self, frame_proc=lambda x: x, target_frames=None, show_frames=True):\n",
    "        frame_no = 0\n",
    "        while True:\n",
    "            st = time.time()\n",
    "            ret, frame = self.cap.read()\n",
    "            frame_no += 1\n",
    "            if not ret:\n",
    "                break\n",
    "            elif target_frames is not None and frame_no not in target_frames:\n",
    "                continue\n",
    "            else:\n",
    "                frame = frame_proc(frame)\n",
    "            \n",
    "            if show_frames:\n",
    "                imshow(frame)\n",
    "            else:\n",
    "                print('processing', frame_no)\n",
    "            \n",
    "            elapsed_time = time.time() - st\n",
    "            if elapsed_time < 1 / self.fps:\n",
    "                time.sleep(1 /self.fps - elapsed_time)\n",
    "    \n",
    "        self.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a0933-e2db-42b4-913f-9031ded26b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回使う動画\n",
    "sample = 'data/sample_0.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a46c77-e1d9-46a8-9721-660af0625c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 試しに再生\n",
    "CvCapture(sample).process_frames()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c388ad-4b00-484d-b0ab-202de577c542",
   "metadata": {},
   "source": [
    "### 1. 姿勢推定により身体の部位の位置を取得\n",
    "\n",
    "1. **姿勢推定により身体の部位の位置を取得**\n",
    "2. 動画からスイング部分を切り出す（本Notebook内では取り扱わない）\n",
    "3. 時系列に位置を並べてデータ化する\n",
    "4. 良いパターンと良くないパターンについて学習する\n",
    "\n",
    "**OpenPoseソースを取得する**\n",
    "\n",
    "※以下コメントアウトを外した上で実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a9099-3ef7-42c2-94b5-54c199b90f1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Hzzone/pytorch-openpose.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3bb199-8e5c-4d66-95b5-4827fa276a54",
   "metadata": {
    "tags": []
   },
   "source": [
    "OpenPoseに必要なライブラリのインストール\n",
    "\n",
    "※以下コメントアウトを外した上で実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13668b26-50f1-4b09-a993-92c64cc66947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -r pytorch-openpose/requirements.txt -t pytorch-openpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c28c6b-1afc-4a4f-8fae-293275678be1",
   "metadata": {
    "tags": []
   },
   "source": [
    "姿勢推定モデルのダウンロード\n",
    "\n",
    "※以下コメントアウトを外した上で実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ce5be-e89c-499d-99a0-2ae3862e5e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget https://www.dropbox.com/sh/7xbup2qsn7vvjxo/AABaYNMvvNVFRWqyDXl7KQUxa/body_pose_model.pth -P pytorch-openpose/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd194a5-84ab-4893-ac21-8f2af3d3fb50",
   "metadata": {
    "tags": []
   },
   "source": [
    "Python実行時のパスを通す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51771d6-b2ea-4109-9909-6c0d9ae75d2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# Pythonのインポートパスにopenposeを追加\n",
    "openpose_path = 'pytorch-openpose'\n",
    "sys.path.append(openpose_path)\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85842852-79ce-489f-88a0-25f423f3b1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src import model\n",
    "from src import util as op_util\n",
    "from src.body import Body\n",
    "\n",
    "__body_estimate = Body('pytorch-openpose/model/body_pose_model.pth')\n",
    "\n",
    "def body_estimation(img):\n",
    "    c, s = __body_estimate(img)\n",
    "    return np.array(c), np.array(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb1d37-e792-42af-80a5-c8a486e7821d",
   "metadata": {},
   "source": [
    "OpenPose-Pythonライブラリから提供された`Body`クラスに姿勢推定モデルを渡して初期化する。  \n",
    "`Body`を実行すると指定した画像の人物およびその姿勢情報を推定して返却する。  \n",
    "ここでは使いやすくするため、この処理を`body_estimation`関数として定義し、以降こちらを使用して画像の姿勢推定を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88fb398-2ade-4543-ab2a-e16a716c871c",
   "metadata": {},
   "source": [
    "**姿勢推定データを見る**\n",
    "\n",
    "例として、動画データの30フレーム目の姿勢推定結果を表示する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91be612-e49e-487d-8b4d-863de9113033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30フレーム目を取り出す\n",
    "frame = CvCapture(sample).get_specified_frame(30)\n",
    "# 姿勢推定\n",
    "candidate, subset = body_estimation(frame)\n",
    "print('Candidate サイズ', candidate.shape)\n",
    "pp(candidate)\n",
    "print('Subset サイズ', subset.shape)\n",
    "pp(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362098e-976f-488d-ae81-a98ca79696c1",
   "metadata": {},
   "source": [
    "**部位データ**\n",
    "\n",
    "OpenPoseの姿勢推定結果には`Candidate`と`Subset`があり、`body_estimation`関数から実行結果として返却される。\n",
    "\n",
    "Candidate\n",
    "* 身体部位の検出情報\n",
    "* サイズは検出された個体数×18（各個体は18個の部位データがある）\n",
    "* 部位データは`[X座標, Y座標, 確度, 検出ID]`\n",
    "* 複数人物が居る画像では複数の部位データがあり、その場合、検出IDが順序通りの数になるとは限らない\n",
    "\n",
    "Subset\n",
    "* 個体の検出情報\n",
    "* サイズは検出された人物等の個体数（各個体には18個の検出IDとスコア）\n",
    "* 検出IDは検出された場合のみ0以上の整数、検出されなければ-1が入る\n",
    "* SubsetのN番目のデータはCandidateのN番目のデータに対応する\n",
    "\n",
    "以降ではこれを**部位データ**と記述する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6380d54-5e75-41db-8aff-cc11acc4799b",
   "metadata": {},
   "source": [
    "**部位データを元画像に描画する処理**\n",
    "\n",
    "CandidateとSubsetで指定された部位を繋げて骨格のように表示する。\n",
    "描画処理のソースコードは別ファイルに記載。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fafe80f-b581-4be5-905a-cd9251d015a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_bodypose(img, candidate, subset):\n",
    "    return op_util.draw_bodypose(copy.deepcopy(img), candidate, subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c721d-211a-4406-b17a-a4d3313d5c1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "試しに実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d57877-272a-4b32-84fe-20419852ac36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_estimation(d):\n",
    "    c, s = body_estimation(d)\n",
    "    # `draw_bodypose`は画像データを変更するため\n",
    "    # 元データを保持したい場合はcopy.deepcopy等で複製したデータを渡す\n",
    "    d = draw_bodypose(d, c, s)\n",
    "    return d\n",
    "\n",
    "# 動画の40～50フレームを5フレームごとに表示\n",
    "CvCapture(sample).process_frames(frame_proc=print_estimation, target_frames=np.arange(40, 51, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0818aa-12d8-4178-9134-21b815e64a01",
   "metadata": {},
   "source": [
    "### 2. 動画からスイング部分を切り出す（本Notebook内では取り扱わない）\n",
    "\n",
    "1. 姿勢推定により身体の部位の位置を取得\n",
    "2. **動画からスイング部分を切り出す（本Notebook内では取り扱わない）**\n",
    "3. 時系列に位置を並べてデータ化する\n",
    "4. 良いパターンと良くないパターンについて学習する\n",
    "\n",
    "スイング開始・終了の判定は条件が多く、動画全体からスイング部分のみ抜き出すコードは長くなるため割愛するが  \n",
    "処理の概要は以下の通り。\n",
    "\n",
    "* スイング先頭判定が真となるまで次のフレームを読み込む\n",
    "* スイング終了判定が真となるまで部位データを取得する\n",
    "* スイング先頭～終了までの代表値として11個のフレームを抽出する\n",
    "* 抽出した部位データを保存する（※スイング判定完成時は、ここで保存のかわりに判定を行う）\n",
    "* 動画が終了するまで上記の処理を続けて行う\n",
    "* 参考：スイング先頭判定：部位データを用いる\n",
    "    * 手首が近い\n",
    "    * 手首の高さが頭に近い\n",
    "    * 手首が一定以上の位置にある\n",
    "    * 腰の左右が離れている（腰がひねられていない）\n",
    "    * 全身が映っている"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b47ab-10e2-485d-bb1c-31571cd46df1",
   "metadata": {},
   "source": [
    "### 3. 時系列に位置を並べてデータ化する\n",
    "\n",
    "1. 姿勢推定により身体の部位の位置を取得\n",
    "2. 動画からスイング部分を切り出す（本Notebook内では取り扱わない）\n",
    "3. **時系列に位置を並べてデータ化する**\n",
    "4. 良いパターンと良くないパターンについて学習する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa89dd4d-f999-47bf-96f1-67fc56118c6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "**スイングデータ**\n",
    "\n",
    "スイング判定モデルの学習に使われる入力データは、部位データを時系列に並べたものとなる。  \n",
    "時系列での保存は動画データのフレームを順次姿勢推定する事で行う。\n",
    "\n",
    "そこで取得データを姿勢推定して保存する処理を定義する。（推定後に描画しているが高速化のためには描画はない方が良い。）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61e60d-476b-47a2-95cb-649ecd772126",
   "metadata": {},
   "source": [
    "まず、実行短縮のためフレーム画像をデータとして保存しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c8742-5ed7-48e4-8c45-f73037dc297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frames = []\n",
    "\n",
    "cap = CvCapture(sample)\n",
    "for i in range(100):\n",
    "    print('.', end='')\n",
    "    frame = cap.get_specified_frame(i)\n",
    "    sample_frames.append(frame)\n",
    "\n",
    "print(len(sample_frames), 'frames')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f149fe5-98d4-4f8a-a729-73eeaa6749e2",
   "metadata": {},
   "source": [
    "**スイング部分の抽出**\n",
    "\n",
    "今回はスイング開始終了を判定する箇所は作らないので、手作業で抽出フレーム番号を調整。  \n",
    "以下のうちから姿勢推定が出来た11フレームのランドマークを抽出する。\n",
    "* 29～40: 振りはじめ付近\n",
    "* 41～52: スイング中～終了\n",
    "\n",
    "姿勢推定が出来たかどうかはsubsetのサイズを見る。(検出できていなければ0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3427e40c-1544-49c0-8364-6dbfe0dbfa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_candidates = []\n",
    "tmp_subsets = []\n",
    "swing_indexes = np.arange(29, 41, 5).tolist() + np.arange(41, 53, 1).tolist()\n",
    "print(swing_indexes)\n",
    "\n",
    "# 動画から指定したフレームのみ抜き出し、描画すると共にスイングデータとして収集する\n",
    "# CvCapture(sample).process_frames(frame_proc=swing.collect_data, target_frames=swing_indexes)\n",
    "for frame_no in swing_indexes:\n",
    "    d = sample_frames[frame_no]\n",
    "    c, s = body_estimation(d)\n",
    "    if s.size == 0: continue\n",
    "    # print(frame_no, '.', end='')\n",
    "    imshow(draw_bodypose(d, c, s))\n",
    "    tmp_candidates.append(c)\n",
    "    tmp_subsets.append(s)\n",
    "\n",
    "print(' collect candidates', len(tmp_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74108d-7811-45dd-9586-375975514d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# スイングのフレーム数を11に調整する\n",
    "candidates = tmp_candidates[1:-3]\n",
    "subsets = tmp_subsets[1:-3]\n",
    "\n",
    "print(len(candidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b396b-d86e-440e-9922-389662d9ba4c",
   "metadata": {},
   "source": [
    "**正確に検出できた部位のみ抽出する**\n",
    "\n",
    "Candidateには18個の部位通りにデータが入っているとは限らず、余分に部位が検出されたり、見えない部分が検出できなかったりする。  \n",
    "以下の例は、Candidateに20個の部位が検出されているが、そのうち2つ(ID=5,17)がSubset側に入っていない(この人物として検出できていない)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6170fdf-3de3-42bd-bb4a-c57bb9a190f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(candidates[0].shape, candidates[0])\n",
    "print(subsets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681f72d-dbf0-40ae-a719-24478d78fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CandidateからSubsetに指定されているデータだけ抽出\n",
    "swing_xy = []\n",
    "for i, subs in enumerate(subsets):\n",
    "    x = subs[0, :18].astype(int)\n",
    "    swing_xy.append(candidates[i][x, :2])\n",
    "\n",
    "swing_xy = np.array(swing_xy)\n",
    "print(swing_xy.shape)\n",
    "print(swing_xy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3050278-bb4d-4e3a-b57e-04b9a481acb9",
   "metadata": {},
   "source": [
    "11フレーム18部位の座標データとなった。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75a8fe-52e6-48a2-ab9d-abb06abde2a9",
   "metadata": {},
   "source": [
    "**部位データ**\n",
    "\n",
    "OpenPoseでは18個の部位を取得するがそのうちスイングで使用する値のみ抽出する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0bbdd1-8179-46db-8ebd-35b3b260365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 部位グループ\n",
    "lg_head = [0] # 鼻\n",
    "lg_upper = [1, 2, 5]  # 中心、右肩、左肩\n",
    "lg_arms = [3, 4, 6, 7]    # 右ひじ、右手首、左ひじ、左手首\n",
    "lg_trunk = [0, 1, 8, 11]  # 鼻、中心、右腰、左腰\n",
    "lg_leg = [9, 10, 12, 13]  # 右ひざ、右足首、左ひざ、左足首\n",
    "# フォーム評価で用いるすべての部位\n",
    "lg_all_ff = sorted(set(lg_head + lg_upper + lg_arms + lg_trunk + lg_leg))\n",
    "print(lg_all_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f90753-72db-4886-8e85-6fa8ea4bc772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidateから使う部分だけ取得する例\n",
    "# 先頭フレームの14か所の座標を表示\n",
    "print(swing_xy[0][lg_all_ff].shape)\n",
    "swing_xy[0][lg_all_ff]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d94aa3-97f9-45f1-abb5-5b08fcd2ab19",
   "metadata": {},
   "source": [
    "**データの正規化**\n",
    "\n",
    "ここまでに得られたスイングの部位座標は画像の左上から横方向と縦方向の位置（ピクセル数）となる。  \n",
    "これには以下の問題がある。\n",
    "* フレーム内における人物の位置や僅かな向きの違いにより値が変わる\n",
    "* 画像サイズにより値が変わる\n",
    "\n",
    "学習データとして使いやすくするため、今回はいくつかの処理により部位座標データを0から1の間に正規化する。  \n",
    "\n",
    "* 人物の位置により値が変わる　⇒ピクセル数の絶対値ではなく、1フレーム目からの変化にする\n",
    "* 向きや画像サイズにより値が変わる　⇒変化率を用いる\n",
    "\n",
    "以下に計算の一部を記述する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc515a-8e76-4749-ba17-7a850be1dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_train_data(data):\n",
    "    # フォーム判定で使用する箇所を抽出\n",
    "    s_data = copy.deepcopy(data)\n",
    "    # 基準位置を0として基準位置からの移動距離に直す\n",
    "    # 先頭データは基準位置(すべて0)なので除外\n",
    "    s_data = s_data[1:] - s_data[0, :]\n",
    "    # 移動距離を、身体の縦サイズとの移動比率に変換\n",
    "    s_data = s_data / 100\n",
    "    # 検出されなかった部位の値NaNを補間する\n",
    "    # 間のNaNは中間値で、端のNaNは隣接値で埋める\n",
    "    return s_data.reshape([s_data.shape[0], math.prod(s_data.shape[1:])])\n",
    "\n",
    "sample_swing_td = to_train_data(swing_xy[:, lg_all_ff])\n",
    "print(sample_swing_td.shape)\n",
    "print(sample_swing_td)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5b9af-55c8-4e51-9a9d-031713679805",
   "metadata": {},
   "source": [
    "これで1つのスイングデータ(11フレーム、14個の座標)が、機械学習用の入力データ(10フレーム、14*2=28個の変化率)となった。  \n",
    "この場合、1つのスイングデータのサイズは**280**となる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be385b92-db62-4c9b-87b0-c1f9704da087",
   "metadata": {},
   "source": [
    "### 4. 良いパターンと良くないパターンについて学習する\n",
    "\n",
    "1. 姿勢推定により身体の部位の位置を取得\n",
    "2. 動画からスイング部分を切り出す（本Notebook内では取り扱わない）\n",
    "3. 時系列に位置を並べてデータ化する\n",
    "4. **良いパターンと良くないパターンについて学習する**\n",
    "\n",
    "3で正規化したスイングデータを多く用意し、そのデータからパターンを学習する。  \n",
    "ここは以下の根拠から回帰分析によりスイングを学習する。\n",
    "\n",
    "* 良いスイングと良くないスイングは一定軌道に近づく(と仮定する)\n",
    "* 良いスイングと良くないスイングのパターンは異なる(と仮定する)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa21b6f-4a6f-4887-8d73-11a07d80cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 良くないと仮定したスイングのプロット(頭部分の横方向移動のみ)\n",
    "d = sample_swing_td\n",
    "plt.scatter(np.arange(d.shape[0]), d[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a57dd-f24d-4ef9-a176-aed428a41fa3",
   "metadata": {},
   "source": [
    "**良いスイングパターンを求める**\n",
    "\n",
    "上のようなフレーム毎の位置変化は、良いスイングと良くないスイングとで統計的に見て異なると考えられる。  \n",
    "\n",
    "そこで、様々なスイングデータを集め、良い=1、良くない=0として、ラベルを付ける。  \n",
    "これを1行1スイングデータとしてCSVファイルとしてまとめる。\n",
    "\n",
    "CSVレイアウトは以下の様になる。\n",
    "\n",
    "**列番号: 項目名**\n",
    "* 0: 1フレーム目の頭x(の位置変化率)\n",
    "* 1: 1フレーム目の頭y\n",
    "* 2: 1フレーム目の中心x\n",
    "* ...\n",
    "* 279: 10フレーム目の左足首y\n",
    "* 280: 良いスイングである(1または0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f6cf42-237a-474d-a5b7-68c7ef361a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_csv('data/train.csv', header=None)\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aaa94c-02e9-429a-a8c4-cb730ff4b72a",
   "metadata": {},
   "source": [
    "281列目に0または1があり、これがその行のスイングが良いか悪いかのラベルとなる。  \n",
    "`train.csv`には、以下の通り、良いスイングのデータも含まれている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c105d5-f7d5-49b3-9be7-a345c69afe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.loc[input_data[280] == 1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f811cf-9fe6-406f-b201-8c12c36347af",
   "metadata": {},
   "source": [
    "参考：OpenPose部位\n",
    "\n",
    "~~~\n",
    "０：頭(鼻)、１：中心(心臓付近)、２：右肩、3：右肘、４：右手首、５：左肩、６：左肘\n",
    "７：左手首、８：右腰、９：右膝、10：右足首、11：左腰、12：左膝、13：左足首\n",
    "14：右目、15：左目、16：右耳、17：左耳\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f759c-3b5a-471f-a7f7-99464516b271",
   "metadata": {},
   "source": [
    "試しにXGBoostによる回帰を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9c84b-15b7-4541-b0e7-db91b01e6482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "print(xgboost.__version__)\n",
    "model = xgboost.XGBRegressor(n_estimators=500, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n",
    "\n",
    "print(input_data.shape)\n",
    "x, y = input_data.values[:, :-1], input_data.values[:, -1]\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72768f-1293-4bcf-84c9-75a0b268016e",
   "metadata": {},
   "source": [
    "先ほどの自分のスイングデータ(学習CSVデータに含まれていない)を判定してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08ba36-eab0-4ef3-bc6f-b5c6a18edcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = sample_swing_td.astype(float)\n",
    "print(d.shape)\n",
    "d = d.reshape(1, 280)\n",
    "print(d.shape)\n",
    "model.predict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd96c0-f233-4d9e-91b0-419aa0e23f24",
   "metadata": {},
   "source": [
    "値は0から1の値を取る。  \n",
    "これを「良いスイング」である確度と考えると、例えば、0.5以上であれば良いスイングと判定する事になる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
